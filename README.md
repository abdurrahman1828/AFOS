# AFOS
 Activation Function Optimization Scheme for Image Classification


 Activation function has a significant impact on the dynamics, convergence, and performance of deep neural networks. The search for a consistent and high-performing activation function has always been a pursuit during deep learning model development. Rectified Linear Unit (ReLU), sigmoid, and tanh are three widely used monotonic activation functions. On top of that, four non-monotonic activation functions such as Swish, Mish, ELU (Exponential Linear Unit), and Gaussian error Linear Unit (GeLU) have also demonstrated good performance in deep learning structures. However, existing activation functions struggle to achieve the best performance in all cases. In this work, we propose an activation function optimization scheme based on the evolutionary approach. Through this optimization framework, we obtain a series of high performing activation functions denoted as Exponential Error Linear Unit (EELU). The optimized activation functions are validated for image classification problems from two perspectives: (1) four state-of-art neural network architectures, such as ResNet50, AlexNet, VGG16, and MobileNet, which cover from computationally heavy to light neural networks and (2) six standard datasets, such as CIFAR10, Imagenette, MNIST, Fashion MNIST, Beans, and Colorectal Histology, which cover from typical machine vision, agricultural image applications to medical image applications. Finally, we statistically investigate the performance of the resultant activation functions generated by the optimization scheme. With a Friedman test, we conclude that the optimization scheme is able to generate activation functions that outperform the existing standard ones in 90.4% cases among 21 different cases studied and EELU-2 is found as the best activation function for image classification generated by the optimization scheme. 